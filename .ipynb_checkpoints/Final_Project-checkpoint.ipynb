{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib.dates import DateFormatter\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "energy = pd.read_csv(\"energy_dataset.csv\")\n",
    "weather = pd.read_csv(\"weather_features.csv\")\n",
    "\n",
    "\n",
    "# -> Checking for null values and types in dataframe.\n",
    "#energy.info()\n",
    "#energy.isna().sum()\n",
    "\n",
    "\n",
    "# -> Temporarly removing \"time\" column to better handle the nulls, will be hading it latter again.\n",
    "time_df = pd.DataFrame(data = energy[\"time\"])\n",
    "#time_df\n",
    "\n",
    "\n",
    "# -> Dropping columns we don't need.\n",
    "energy = energy.drop([\"price day ahead\", \"time\", \"generation fossil coal-derived gas\", \"generation fossil oil shale\", \"generation fossil peat\", \"generation geothermal\", \"generation marine\", \"generation wind offshore\", \"generation hydro pumped storage aggregated\", \"forecast wind offshore eday ahead\"], axis = 1)\n",
    "\n",
    "\n",
    "# -> Impute missing values function.\n",
    "def impute_missing_values(energy):\n",
    "    for column in energy:\n",
    "        energy.loc[:,column] = np.where(energy[column].isna(), energy[column].mean(), energy[column])\n",
    "    return energy\n",
    "impute_missing_values(energy)\n",
    "\n",
    "\n",
    "# -> Divided all values by all 5 cities.\n",
    "energy = energy/5\n",
    "\n",
    "\n",
    "# -> Hading back the colum to our cleaned dataset.\n",
    "energy = time_df.join(energy)\n",
    "\n",
    "\n",
    "# -> Droping collums we don't need.\n",
    "weather = weather.drop([\"weather_icon\", \"weather_id\", \"weather_description\"], axis = 1)\n",
    "\n",
    "\n",
    "# -> Merging data frames.\n",
    "sample = energy.merge(weather, left_on = \"time\", right_on = \"dt_iso\")\n",
    "\n",
    "\n",
    "# -> Droping \"duplicade\" columns.\n",
    "sample = sample.drop([\"dt_iso\"], axis =1)\n",
    "\n",
    "\n",
    "# -> Uniforming columns names.\n",
    "sample.columns = sample.columns.str.replace(\" \", \"_\")\n",
    "\n",
    "\n",
    "# -> OneHotEncoding Function.\n",
    "def oneHotEncode(sample, colNames):\n",
    "    for col in colNames:\n",
    "        if( sample[col].dtype == np.dtype(\"object\")):\n",
    "            dummies = pd.get_dummies(sample[col], prefix = col)\n",
    "            sample = pd.concat([sample,dummies], axis = 1)\n",
    "# drop the encoded column\n",
    "            sample.drop([col],axis = 1 , inplace = True)\n",
    "    return sample\n",
    "\n",
    "\n",
    "# -> Checking uniques values for one hot encoding.\n",
    "#sample[\"weather_main\"].unique()\n",
    "sample = oneHotEncode(sample, [\"weather_main\"])\n",
    "\n",
    "\n",
    "# -> Temporarly removing \"city_name\" column to better handle data for the model.\n",
    "city_name_df = pd.DataFrame(data = sample[\"city_name\"])\n",
    "####### DON'T FORGET TO ADD IN THE LAST PART OF THE CODE !!!!! ###########\n",
    "sample = sample.drop([\"city_name\"], axis = 1)\n",
    "\n",
    "\n",
    "# -> Passing time-object to datetime type.\n",
    "sample[\"time\"] = pd.to_datetime(sample[\"time\"], utc = True)\n",
    "\n",
    "\n",
    "# -> Seperating time values to single colums.\n",
    "sample[\"Month\"] = pd.to_datetime(sample[\"time\"]).dt.month\n",
    "sample[\"Day\"] = pd.to_datetime(sample[\"time\"]).dt.day\n",
    "sample[\"Hours\"] = pd.to_datetime(sample[\"time\"]).dt.hour\n",
    "\n",
    "\n",
    "# -> Droping time colum.\n",
    "sample = sample.drop([\"time\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ ADD CITY NAMES AT THE END #####################\n",
    "\n",
    "# -> Hading back the colum to our cleaned dataset.\n",
    "#sample = city_name_df.join(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap Energy\n",
    "\n",
    "#plt.figure(figsize=(16, 6))\n",
    "#heatmap = sns.heatmap(energy.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "#heatmap.set_title('Correlation Heatmap Energy', fontdict={'fontsize':18}, pad=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap Weather\n",
    "\n",
    "#plt.figure(figsize=(16, 6))\n",
    "#heatmap = sns.heatmap(weather.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "#heatmap.set_title('Correlation Heatmap Weather', fontdict={'fontsize':18}, pad=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split Train, Test\n",
    "\n",
    "features = sample.drop([\"price_actual\"], axis=1)\n",
    "\n",
    "# Features train set\n",
    "X_train = features[:124877]\n",
    "\n",
    "# Label train set\n",
    "y_train = sample[:124877].price_actual\n",
    "\n",
    "# Features test set\n",
    "X_test = features[124877:]\n",
    "\n",
    "# Label test set\n",
    "y_test = sample[124877:].price_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model train and selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 44)                1980      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                1350      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 4,291\n",
      "Trainable params: 4,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(44, kernel_initializer='normal', input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(30, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(30, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3122/3122 [==============================] - 3s 876us/step - loss: 1.8615 - mean_absolute_error: 1.8615 - val_loss: 1.2134 - val_mean_absolute_error: 1.2134\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.21335, saving model to Weights-001--1.21335.hdf5\n",
      "Epoch 2/20\n",
      "3122/3122 [==============================] - 2s 596us/step - loss: 1.6347 - mean_absolute_error: 1.6347 - val_loss: 1.2335 - val_mean_absolute_error: 1.2335\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.21335\n",
      "Epoch 3/20\n",
      "3122/3122 [==============================] - 2s 598us/step - loss: 1.5547 - mean_absolute_error: 1.5547 - val_loss: 1.1721 - val_mean_absolute_error: 1.1721\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.21335 to 1.17211, saving model to Weights-003--1.17211.hdf5\n",
      "Epoch 4/20\n",
      "3122/3122 [==============================] - 2s 603us/step - loss: 1.5204 - mean_absolute_error: 1.5204 - val_loss: 1.3155 - val_mean_absolute_error: 1.3155\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.17211\n",
      "Epoch 5/20\n",
      "3122/3122 [==============================] - 2s 592us/step - loss: 1.5035 - mean_absolute_error: 1.5035 - val_loss: 1.1877 - val_mean_absolute_error: 1.1877\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.17211\n",
      "Epoch 6/20\n",
      "3122/3122 [==============================] - 2s 598us/step - loss: 1.4651 - mean_absolute_error: 1.4651 - val_loss: 1.2391 - val_mean_absolute_error: 1.2391\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.17211\n",
      "Epoch 7/20\n",
      "3122/3122 [==============================] - 2s 598us/step - loss: 1.4400 - mean_absolute_error: 1.4400 - val_loss: 1.3406 - val_mean_absolute_error: 1.3406\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.17211\n",
      "Epoch 8/20\n",
      "3122/3122 [==============================] - 2s 598us/step - loss: 1.4216 - mean_absolute_error: 1.4216 - val_loss: 1.5149 - val_mean_absolute_error: 1.5149\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.17211\n",
      "Epoch 9/20\n",
      "3122/3122 [==============================] - 2s 618us/step - loss: 1.4111 - mean_absolute_error: 1.4111 - val_loss: 1.3052 - val_mean_absolute_error: 1.3052\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.17211\n",
      "Epoch 10/20\n",
      "3122/3122 [==============================] - 2s 600us/step - loss: 1.3899 - mean_absolute_error: 1.3899 - val_loss: 1.3365 - val_mean_absolute_error: 1.3365\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.17211\n",
      "Epoch 11/20\n",
      "3122/3122 [==============================] - 2s 601us/step - loss: 1.3840 - mean_absolute_error: 1.3840 - val_loss: 1.3688 - val_mean_absolute_error: 1.3688\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.17211\n",
      "Epoch 12/20\n",
      "3122/3122 [==============================] - 2s 597us/step - loss: 1.3863 - mean_absolute_error: 1.3863 - val_loss: 1.4425 - val_mean_absolute_error: 1.4425\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.17211\n",
      "Epoch 13/20\n",
      "3122/3122 [==============================] - 2s 597us/step - loss: 1.3549 - mean_absolute_error: 1.3549 - val_loss: 1.3229 - val_mean_absolute_error: 1.3229\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.17211\n",
      "Epoch 14/20\n",
      "3122/3122 [==============================] - 2s 594us/step - loss: 1.3476 - mean_absolute_error: 1.3476 - val_loss: 1.4654 - val_mean_absolute_error: 1.4654\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.17211\n",
      "Epoch 15/20\n",
      "3122/3122 [==============================] - 2s 598us/step - loss: 1.3472 - mean_absolute_error: 1.3472 - val_loss: 1.3522 - val_mean_absolute_error: 1.3522\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.17211\n",
      "Epoch 16/20\n",
      "3122/3122 [==============================] - 2s 597us/step - loss: 1.3364 - mean_absolute_error: 1.3364 - val_loss: 1.3613 - val_mean_absolute_error: 1.3613\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.17211\n",
      "Epoch 17/20\n",
      "3122/3122 [==============================] - 2s 593us/step - loss: 1.3303 - mean_absolute_error: 1.3303 - val_loss: 1.4686 - val_mean_absolute_error: 1.4686\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.17211\n",
      "Epoch 18/20\n",
      "3122/3122 [==============================] - 2s 654us/step - loss: 1.3063 - mean_absolute_error: 1.3063 - val_loss: 2.0051 - val_mean_absolute_error: 2.0051\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.17211\n",
      "Epoch 19/20\n",
      "3122/3122 [==============================] - 2s 607us/step - loss: 1.3070 - mean_absolute_error: 1.3070 - val_loss: 1.4824 - val_mean_absolute_error: 1.4824\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.17211\n",
      "Epoch 20/20\n",
      "3122/3122 [==============================] - 2s 611us/step - loss: 1.2810 - mean_absolute_error: 1.2810 - val_loss: 1.4531 - val_mean_absolute_error: 1.4531\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.17211\n",
      "KERAS: R2 : -0.308534, RMSE : 8.005045\n"
     ]
    }
   ],
   "source": [
    "NN_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)\n",
    "\n",
    "y_pred = NN_model.predict( X_test )\n",
    "\n",
    "r2 = r2_score( y_test, y_pred )\n",
    "\n",
    "rmse = mean_squared_error( y_test, y_pred )\n",
    "\n",
    "print( \"R2 : {0:f}, RMSE : {1:f}\".format( r2, rmse ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.465047317656542\n"
     ]
    }
   ],
   "source": [
    "tree_model = DecisionTreeRegressor()\n",
    "\n",
    "scores = cross_val_score(tree_model, X_train, y_train, cv=5, scoring = \"neg_mean_absolute_error\")\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KERAS: R2 : -0.585557, RMSE : 9.699753\n"
     ]
    }
   ],
   "source": [
    "tree_model.fit( X_train, y_train )\n",
    "\n",
    "y_pred = tree_model.predict( X_test )\n",
    "\n",
    "r2 = r2_score( y_test, y_pred )\n",
    "\n",
    "rmse = mean_squared_error( y_test, y_pred )\n",
    "\n",
    "print( \"R2 : {0:f}, RMSE : {1:f}\".format( r2, rmse ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
